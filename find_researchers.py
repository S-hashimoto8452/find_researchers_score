# Run with:  streamlit run ResearcherScreening_v4_app.py
# Requirements: pip install streamlit requests pandas python-dateutil

import re
import time
from typing import List, Dict, Any, Tuple

import pandas as pd
import requests
import streamlit as st

# Safety: avoid NameError if legacy blocks still reference `run`
run = False

st.set_page_config(page_title="InsighTCROSS Literature Scorer v4", layout="wide")

# --- ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼å‡¦ç†ï¼ˆå …ç‰¢åŒ–ç‰ˆï¼‰ ---
def check_password():
    """
    GitHub/Streamlit Cloudå‰æã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ä¿è­·ã€‚
    - .streamlit/secrets.toml ã® [passwords].app_password ã‚’ä½¿ç”¨ï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰
    """
    # secrets ã®è¨­å®šæ¼ã‚Œãƒã‚§ãƒƒã‚¯
    if "passwords" not in st.secrets or "app_password" not in st.secrets["passwords"]:
        st.error("ã‚¢ãƒ—ãƒªè¨­å®šã‚¨ãƒ©ãƒ¼ï¼š.streamlit/secrets.toml ã« [passwords].app_password ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # çŠ¶æ…‹ç®¡ç†
    if "password_correct" not in st.session_state:
        st.session_state["password_correct"] = False
    if "pw_attempts" not in st.session_state:
        st.session_state["pw_attempts"] = 0

    # æœªèªè¨¼ï¼šãƒ•ã‚©ãƒ¼ãƒ 
    if not st.session_state["password_correct"]:
        with st.form("login_form", clear_on_submit=False):
            password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")
            submitted = st.form_submit_button("ãƒ­ã‚°ã‚¤ãƒ³")

        if submitted:
            if password == st.secrets["passwords"]["app_password"]:
                st.session_state["password_correct"] = True
                st.session_state["pw_attempts"] = 0
                st.rerun()
            else:
                st.session_state["pw_attempts"] += 1
                st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™ã€‚")
                if st.session_state["pw_attempts"] >= 5:
                    st.warning("è©¦è¡Œå›æ•°ãŒå¤šã™ãã¾ã™ã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ç©ºã‘ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
                    st.stop()
        return False

    return True

# -------------------- Helpers --------------------
EPMC_SEARCH_URL = "https://www.ebi.ac.uk/europepmc/webservices/rest/search"

ARTICLE_TYPE_MAP = {
    "Clinical Trial": "Clinical Trial",
    "Meta-Analysis": "Meta-Analysis",
    "Randomized Controlled Trial": "Randomized Controlled Trial",
    "Review": "Review",
    "Systematic Review": "Systematic Review",
}

def build_query(
    disease: str,
    country: str,
    year_from: int,
    year_to: int,
    department: str,
    keywords: str,
    ta_abstract: bool,
    ta_free_full: bool,
    ta_full_text: bool,
    atypes: List[str],
    src_med: bool,
    src_pmc: bool,
    src_ppr: bool,
    excl_ppr: bool,
) -> str:
    parts = []

    def sanitize(x: str) -> str:
        x = re.sub(r'["â€œâ€â€â€ŸÂ«Â»â€¹â€ºã€Œã€ã€ã€ï¼‚]', '', x.strip())
        return re.sub(r"\s+", " ", x)

    # Disease: ãƒ•ãƒ¬ãƒ¼ã‚ºä¸€è‡´ + å…¨èªANDï¼ˆå–ã‚Šã“ã¼ã—æ¸›ï¼‰
    if disease:
        dz = sanitize(disease)
        tokens = [t for t in dz.split(" ") if t]
        phrase = f'"{dz}"'
        if len(tokens) > 1:
            and_block = " AND ".join([f'"{t}"' for t in tokens])
            parts.append(
                f'(TITLE:{phrase} OR ABSTRACT:{phrase} OR TITLE:({and_block}) OR ABSTRACT:({and_block}))'
            )
        else:
            parts.append(f'(TITLE:{phrase} OR ABSTRACT:{phrase})')

    # Department/Division in affiliationï¼ˆå’Œè‹±ãƒ»è¡¨è¨˜æºã‚Œã‚’å°‘ã—å¸åï¼‰
    if department:
        dept_tokens = [sanitize(t) for t in re.split(r",|ã€|;|/|\|", department) if sanitize(t)]
        if dept_tokens:
            exp = []
            for t in dept_tokens:
                exp.extend(
                    [
                        f'AFF:"{t}"',
                        f'AFF:"Department of {t}"',
                        f'AFF:"Division of {t}"',
                        f'AFF:"Dept. of {t}"',
                        f'AFF:"{t} Department"',
                        f'AFF:"{t} Division"',
                        f'AFF:"{t} Unit"',
                        f'AFF:"{t}ç§‘"',
                        f'AFF:"{t}éƒ¨"',
                        f'AFF:"{t}è¬›åº§"',
                    ]
                )
            parts.append("(" + " OR ".join(exp) + ")")

    # Free keywords in Title/Abstract
    if keywords:
        toks = [sanitize(t) for t in re.split(r",|ã€|;|/", keywords) if sanitize(t)]
        if toks:
            or_block = " OR ".join([f'"{t}"' for t in toks])
            parts.append(f"(TITLE:({or_block}) OR ABSTRACT:({or_block}))")

    if country:
        parts.append(f'AFF:"{sanitize(country)}"')

    # Date range: FIRST_PDATE ã‚’ä½¿ã£ã¦å³å¯†ã«ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆYYYY-MM-DDï¼‰
    parts.append(f"FIRST_PDATE:[{int(year_from)}-01-01 TO {int(year_to)}-12-31]")

    if ta_abstract:
        parts.append("HAS_ABSTRACT:Y")
    if ta_free_full:
        parts.append("OPEN_ACCESS:Y")
    if ta_full_text:
        parts.append("HAS_FULL_TEXT:Y")

    if atypes:
        mapped = [ARTICLE_TYPE_MAP.get(a) for a in atypes if ARTICLE_TYPE_MAP.get(a)]
        if mapped:
            typ_block = " OR ".join([f'"{t}"' for t in mapped])
            parts.append(f"PUB_TYPE:({typ_block})")

    # Sources
    source_tokens = []
    if src_med:
        source_tokens.append("SRC:MED")
    if src_pmc:
        source_tokens.append("SRC:PMC")
    if src_ppr:
        source_tokens.append("SRC:PPR")
    if source_tokens:
        parts.append("(" + " OR ".join(source_tokens) + ")")
    if excl_ppr:
        parts.append("NOT SRC:PPR")

    return " AND ".join(parts) if parts else "*"

def extract_department(aff: str) -> str:
    if not aff:
        return ""
    m = re.search(r"(Department of [^.;|]+|Dept\. of [^.;|]+|Division of [^.;|]+|ç§‘[^ï¼›ã€‚|]+)", aff, flags=re.I)
    return m.group(0) if m else ""

def fetch_eupmc_all(query: str, max_rows: int = 5000, synonym: bool = False) -> Tuple[List[Dict[str, Any]], int]:
    """æœ€å¤§ max_rows ã¾ã§å–å¾—ã—ã€(items, hitCount) ã‚’è¿”ã™ï¼ˆAPIã‚­ãƒ¼ä¸è¦ã®Europe PMCï¼‰"""
    results = []
    cursor = "*"
    page_size = 1000 if max_rows >= 1000 else max_rows
    fetched = 0
    total_hit_count = 0

    while fetched < max_rows:
        params = {
            "query": query,
            "format": "json",
            "resultType": "core",
            "pageSize": page_size,
            "cursorMark": cursor,
            "synonym": str(synonym).lower(),
        }
        r = requests.get(EPMC_SEARCH_URL, params=params, timeout=60)
        r.raise_for_status()
        j = r.json()
        if not total_hit_count:
            total_hit_count = int(j.get("hitCount", 0))
        items = j.get("resultList", {}).get("result", [])
        if not items:
            break
        results.extend(items)
        fetched += len(items)
        cursor = j.get("nextCursorMark") or cursor
        if not j.get("nextCursorMark") or len(items) < page_size:
            break
        time.sleep(0.2)

    return results[:max_rows], total_hit_count

def get_hit_count(query: str, synonym: bool = False) -> int:
    params = {
        "query": query,
        "format": "json",
        "resultType": "core",
        "pageSize": 1,
        "synonym": str(synonym).lower(),
    }
    r = requests.get(EPMC_SEARCH_URL, params=params, timeout=30)
    r.raise_for_status()
    return int(r.json().get("hitCount", 0))

def pick_main_affiliation(aff_value) -> str:
    if not aff_value:
        return ""
    if isinstance(aff_value, list):
        parts = [a for a in aff_value if a]
        if not parts:
            return ""
        first = parts[0]
    else:
        first = str(aff_value)
    return re.split(r"\s*\|\s*", first)[0].strip()

# ---- ãƒ­ãƒ¼ãƒå­—â†’ã‚«ã‚¿ã‚«ãƒŠï¼ˆç°¡æ˜“ãƒ˜ãƒœãƒ³å¼ï¼‰----
VOWELS = set("aeiou")

TRI_MAP = {
    "kya":"ã‚­ãƒ£","kyu":"ã‚­ãƒ¥","kyo":"ã‚­ãƒ§",
    "gya":"ã‚®ãƒ£","gyu":"ã‚®ãƒ¥","gyo":"ã‚®ãƒ§",
    "sha":"ã‚·ãƒ£","shu":"ã‚·ãƒ¥","sho":"ã‚·ãƒ§",
    "cha":"ãƒãƒ£","chu":"ãƒãƒ¥","cho":"ãƒãƒ§",
    "ja":"ã‚¸ãƒ£","ju":"ã‚¸ãƒ¥","jo":"ã‚¸ãƒ§",
    "nya":"ãƒ‹ãƒ£","nyu":"ãƒ‹ãƒ¥","nyo":"ãƒ‹ãƒ§",
    "hya":"ãƒ’ãƒ£","hyu":"ãƒ’ãƒ¥","hyo":"ãƒ’ãƒ§",
    "mya":"ãƒŸãƒ£","myu":"ãƒŸãƒ¥","myo":"ãƒŸãƒ§",
    "rya":"ãƒªãƒ£","ryu":"ãƒªãƒ¥","ryo":"ãƒªãƒ§",
    "bya":"ãƒ“ãƒ£","byu":"ãƒ“ãƒ¥","byo":"ãƒ“ãƒ§",
    "pya":"ãƒ”ãƒ£","pyu":"ãƒ”ãƒ¥","pyo":"ãƒ”ãƒ§",
    "tya":"ãƒãƒ£","tyu":"ãƒãƒ¥","tyo":"ãƒãƒ§",
    "dya":"ã‚¸ãƒ£","dyu":"ã‚¸ãƒ¥","dyo":"ã‚¸ãƒ§",
}

DI_MAP = {
    "shi":"ã‚·","chi":"ãƒ","tsu":"ãƒ„","fu":"ãƒ•","ji":"ã‚¸",
    "ti":"ãƒ†ã‚£","di":"ãƒ‡ã‚£","tu":"ãƒˆã‚¥","du":"ãƒ‰ã‚¥",
    "je":"ã‚¸ã‚§","che":"ãƒã‚§","she":"ã‚·ã‚§",
    "ci":"ã‚·","ce":"ã‚»","wi":"ã‚¦ã‚£","we":"ã‚¦ã‚§","wo":"ãƒ²",
    "ja":"ã‚¸ãƒ£","ju":"ã‚¸ãƒ¥","jo":"ã‚¸ãƒ§",
}

BASE = {
    "a":"ã‚¢","i":"ã‚¤","u":"ã‚¦","e":"ã‚¨","o":"ã‚ª",
    "ka":"ã‚«","ki":"ã‚­","ku":"ã‚¯","ke":"ã‚±","ko":"ã‚³",
    "ga":"ã‚¬","gi":"ã‚®","gu":"ã‚°","ge":"ã‚²","go":"ã‚´",
    "sa":"ã‚µ","si":"ã‚·","su":"ã‚¹","se":"ã‚»","so":"ã‚½",
    "za":"ã‚¶","zi":"ã‚¸","zu":"ã‚º","ze":"ã‚¼","zo":"ã‚¾",
    "ta":"ã‚¿","ti":"ãƒ†ã‚£","tu":"ãƒˆã‚¥","te":"ãƒ†","to":"ãƒˆ",
    "da":"ãƒ€","di":"ãƒ‡ã‚£","du":"ãƒ‰ã‚¥","de":"ãƒ‡","do":"ãƒ‰",
    "na":"ãƒŠ","ni":"ãƒ‹","nu":"ãƒŒ","ne":"ãƒ","no":"ãƒ",
    "ha":"ãƒ","hi":"ãƒ’","hu":"ãƒ•","he":"ãƒ˜","ho":"ãƒ›",
    "ba":"ãƒ","bi":"ãƒ“","bu":"ãƒ–","be":"ãƒ™","bo":"ãƒœ",
    "pa":"ãƒ‘","pi":"ãƒ”","pu":"ãƒ—","pe":"ãƒš","po":"ãƒ",
    "ma":"ãƒ","mi":"ãƒŸ","mu":"ãƒ ","me":"ãƒ¡","mo":"ãƒ¢",
    "ra":"ãƒ©","ri":"ãƒª","ru":"ãƒ«","re":"ãƒ¬","ro":"ãƒ­",
    "ya":"ãƒ¤","yu":"ãƒ¦","yo":"ãƒ¨",
    "wa":"ãƒ¯","we":"ã‚¦ã‚§","wo":"ãƒ²",
    "va":"ãƒ´ã‚¡","vi":"ãƒ´ã‚£","vu":"ãƒ´","ve":"ãƒ´ã‚§","vo":"ãƒ´ã‚©",
    "fa":"ãƒ•ã‚¡","fi":"ãƒ•ã‚£","fe":"ãƒ•ã‚§","fo":"ãƒ•ã‚©",
    "la":"ãƒ©","li":"ãƒª","lu":"ãƒ«","le":"ãƒ¬","lo":"ãƒ­",
    "ca":"ã‚«","ci":"ã‚·","cu":"ã‚¯","ce":"ã‚»","co":"ã‚³",
    "qa":"ã‚«","qi":"ã‚­","qu":"ã‚¯","qe":"ã‚±","qo":"ã‚³",
}

MACRON = str.maketrans({"Ä":"aa","Ä«":"ii","Å«":"uu","Ä“":"ee","Å":"ou","Ã¢":"aa","Ã®":"ii","Ã»":"uu","Ãª":"ee","Ã´":"ou"})

# å€‹åˆ¥ä¸Šæ›¸ãï¼ˆå§“ï¼‰
KATAKANA_LASTNAME_OVERRIDE = {
    "amano": "ã‚¢ãƒãƒ",
    "ando": "ã‚¢ãƒ³ãƒ‰ã‚¦",
    "domei": "ãƒ‰ã‚¦ãƒ¡ã‚¤",
    "gohbara": "ã‚´ã‚¦ãƒãƒ©",
    "goriki": "ã‚´ã‚¦ãƒªã‚­",
    "hanyu": "ãƒãƒ‹ãƒ¥ã‚¦",
    "homma": "ãƒ›ãƒ³ãƒ",
    "honye": "ãƒ›ãƒ³ã‚¨",
    "ito": "ã‚¤ãƒˆã‚¦",
    "jujo": "ã‚¸ãƒ¥ã‚¦ã‚¸ãƒ§ã‚¦",
    "kato": "ã‚«ãƒˆã‚¦",
    "kohro": "ã‚³ã‚¦ãƒ­",
    "kohsaka": "ã‚³ã‚¦ã‚µã‚«",
    "kozuma": "ã‚³ã‚¦ã‚ºãƒ",
    "mitsudo": "ãƒŸãƒ„ãƒ‰ã‚¦",
    "morino": "ãƒ¢ãƒªãƒ",
    "nanasato": "ãƒŠãƒŠã‚µãƒˆ",
    "obayashi": "ã‚ªã‚ªãƒãƒ¤ã‚·",
    "ohi": "ã‚ªã‚ªã‚¤",
    "ohara": "ã‚ªã‚ªãƒãƒ©",
    "ohashi": "ã‚ªã‚ªãƒã‚·",
    "ohata": "ã‚ªã‚ªãƒã‚¿",
    "ohba": "ã‚ªã‚ªãƒ",
    "ohguchi": "ã‚ªã‚ªã‚°ãƒ",
    "ohguro": "ã‚ªã‚ªã‚°ãƒ­",
    "ohishi": "ã‚ªã‚ªã‚¤ã‚·",
    "ohkawa": "ã‚ªã‚ªã‚«ãƒ¯",
    "ohkita": "ã‚ªã‚ªã‚­ã‚¿",
    "ohkubo": "ã‚ªã‚ªã‚¯ãƒœ",
    "ohmori": "ã‚ªã‚ªãƒ¢ãƒª",
    "ohnishi": "ã‚ªã‚ªãƒ‹ã‚·",
    "ohno": "ã‚ªã‚ªãƒ",
    "ohta": "ã‚ªã‚ªã‚¿",
    "ohtani": "ã‚ªã‚ªã‚¿ãƒ‹",
    "ohtomo": "ã‚ªã‚ªãƒˆãƒ¢",
    "ohsaka": "ã‚ªã‚ªã‚µã‚«",
    "ohsawa": "ã‚ªã‚ªã‚µãƒ¯",
    "ohshiro": "ã‚ªã‚ªã‚·ãƒ­",
    "ohshima": "ã‚ªã‚ªã‚·ãƒ",
    "ohsugi": "ã‚ªã‚ªã‚¹ã‚®",
    "ohsumi": "ã‚ªã‚ªã‚¹ãƒŸ",
    "ohuchi": "ã‚ªã‚ªã‚¦ãƒ",
    "ohwada": "ã‚ªã‚ªãƒ¯ãƒ€",
    "ohya": "ã‚ªã‚ªãƒ¤",
    "oiwa": "ã‚ªã‚ªã‚¤ãƒ¯",
    "oishi": "ã‚ªã‚ªã‚¤ã‚·",
    "oizumi": "ã‚ªã‚ªã‚¤ã‚ºãƒŸ",
    "okubo": "ã‚ªã‚ªã‚¯ãƒœ",
    "okura": "ã‚ªã‚ªã‚¯ãƒ©",
    "omori": "ã‚ªã‚ªãƒ¢ãƒª",
    "omura": "ã‚ªã‚ªãƒ ãƒ©",
    "onishi": "ã‚ªã‚ªãƒ‹ã‚·",
    "ono": "ã‚ªãƒ",
    "ooba": "ã‚ªã‚ªãƒ",
    "oobayashi": "ã‚ªã‚ªãƒãƒ¤ã‚·",
    "oochi": "ã‚ªã‚ªãƒ",
    "oodate": "ã‚ªã‚ªãƒ€ãƒ†",
    "oohara": "ã‚ªã‚ªãƒãƒ©",
    "oohashi": "ã‚ªã‚ªãƒã‚·",
    "ooguchi": "ã‚ªã‚ªã‚°ãƒ",
    "ooguro": "ã‚ªã‚ªã‚°ãƒ­",
    "ookawa": "ã‚ªã‚ªã‚«ãƒ¯",
    "ookita": "ã‚ªã‚ªã‚­ã‚¿",
    "ookubo": "ã‚ªã‚ªã‚¯ãƒœ",
    "oomori": "ã‚ªã‚ªãƒ¢ãƒª",
    "oomura": "ã‚ªã‚ªãƒ ãƒ©",
    "oonishi": "ã‚ªã‚ªãƒ‹ã‚·",
    "oono": "ã‚ªã‚ªãƒ",
    "oosaka": "ã‚ªã‚ªã‚µã‚«",
    "oosawa": "ã‚ªã‚ªã‚µãƒ¯",
    "ooshima": "ã‚ªã‚ªã‚·ãƒ",
    "oosugi": "ã‚ªã‚ªã‚¹ã‚®",
    "oouchi": "ã‚ªã‚ªã‚¦ãƒ",
    "oowada": "ã‚ªã‚ªãƒ¯ãƒ€",
    "ooya": "ã‚ªã‚ªãƒ¤",
    "ooi": "ã‚ªã‚ªã‚¤",
    "osaka": "ã‚ªã‚ªã‚µã‚«",
    "oshima": "ã‚ªã‚ªã‚·ãƒ",
    "osumi": "ã‚ªã‚ªã‚¹ãƒŸ",
    "ota": "ã‚ªã‚ªã‚¿",
    "otake": "ã‚ªã‚ªã‚¿ã‚±",
    "otomo": "ã‚ªã‚ªãƒˆãƒ¢",
    "otsuba": "ã‚ªã‚ªãƒ„ãƒ",
    "otsubo": "ã‚ªã‚ªãƒ„ãƒœ",
    "otsuka": "ã‚ªã‚ªãƒ„ã‚«",
    "otsuki": "ã‚ªã‚ªãƒ„ã‚­",
    "oyama": "ã‚ªãƒ¤ãƒ",
    "saito": "ã‚µã‚¤ãƒˆã‚¦",
    "sato": "ã‚µãƒˆã‚¦",
    "shindo": "ã‚·ãƒ³ãƒ‰ã‚¦",
    "sudo": "ã‚¹ãƒ‰ã‚¦",
}

# å€‹åˆ¥ä¸Šæ›¸ãï¼ˆå / FirstNameï¼‰
KATAKANA_FIRSTNAME_OVERRIDE = {
    "asataro": "ã‚¢ã‚µã‚¿ãƒ­ã‚¦",
    "eiichiro": "ã‚¨ã‚¤ã‚¤ãƒãƒ­ã‚¦",
    "eizo": "ã‚¨ã‚¤ã‚¾ã‚¦",
    "go": "ã‚´ã‚¦",
    "goro": "ã‚´ãƒ­ã‚¦",
    "ichitaro": "ã‚¤ãƒã‚¿ãƒ­ã‚¦",
    "issei": "ã‚¤ãƒƒã‚»ã‚¤",
    "itsuro": "ã‚¤ãƒ„ãƒ­ã‚¦",
    "jiro": "ã‚¸ãƒ­ã‚¦",
    "jo": "ã‚¸ãƒ§ã‚¦",
    "joji": "ã‚¸ãƒ§ã‚¦ã‚¸",
    "jun": "ã‚¸ãƒ¥ãƒ³",
    "jun-ichi": "ã‚¸ãƒ¥ãƒ³ã‚¤ãƒ",
    "junichi": "ã‚¸ãƒ¥ãƒ³ã‚¤ãƒ",
    "junko": "ã‚¸ãƒ¥ãƒ³ã‚³",
    "junya": "ã‚¸ãƒ¥ãƒ³ãƒ¤",
    "ken-ichi": "ã‚±ãƒ³ã‚¤ãƒ",
    "kenichi": "ã‚±ãƒ³ã‚¤ãƒ",
    "kenichiro": "ã‚±ãƒ³ã‚¤ãƒãƒ­ã‚¦",
    "kenya": "ã‚±ãƒ³ãƒ¤",
    "kentaro": "ã‚±ãƒ³ã‚¿ãƒ­ã‚¦",
    "ko": "ã‚³ã‚¦",
    "koh": "ã‚³ã‚¦",
    "kohei": "ã‚³ã‚¦ãƒ˜ã‚¤",
    "koichi": "ã‚³ã‚¦ã‚¤ãƒ",
    "koichiro": "ã‚³ã‚¦ã‚¤ãƒãƒ­ã‚¦",
    "koji": "ã‚³ã‚¦ã‚¸",
    "koki": "ã‚³ã‚¦ã‚­",
    "kosei": "ã‚³ã‚¦ã‚»ã‚¤",
    "koshi": "ã‚³ã‚¦ã‚·",
    "koshiro": "ã‚³ã‚¦ã‚·ãƒ­ã‚¦",
    "kosuke": "ã‚³ã‚¦ã‚¹ã‚±",
    "kota": "ã‚³ã‚¦ã‚¿",
    "kotaro": "ã‚³ã‚¦ã‚¿ãƒ­ã‚¦",
    "kyohei": "ã‚­ãƒ§ã‚¦ãƒ˜ã‚¤",
    "kyoko": "ã‚­ãƒ§ã‚¦ã‚³",
    "rensuke": "ãƒ¬ãƒ³ã‚¹ã‚±",
    "ryo": "ãƒªãƒ§ã‚¦",
    "ryohei": "ãƒªãƒ§ã‚¦ãƒ˜ã‚¤",
    "ryoichi": "ãƒªãƒ§ã‚¦ã‚¤ãƒ",
    "ryoji": "ãƒªãƒ§ã‚¦ã‚¸",
    "ryoko": "ãƒªãƒ§ã‚¦ã‚³",
    "ryoma": "ãƒªãƒ§ã‚¦ãƒ",
    "ryosuke": "ãƒªãƒ§ã‚¦ã‚¹ã‚±",
    "ryozo": "ãƒªãƒ§ã‚¦ã‚¾ã‚¦",
    "ryuichi": "ãƒªãƒ¥ã‚¦ã‚¤ãƒ",
    "ryuki": "ãƒªãƒ¥ã‚¦ã‚­",
    "ryusuke": "ãƒªãƒ¥ã‚¦ã‚¹ã‚±",
    "sho": "ã‚·ãƒ§ã‚¦",
    "shohei": "ã‚·ãƒ§ã‚¦ãƒ˜ã‚¤",
    "shoichi": "ã‚·ãƒ§ã‚¦ã‚¤ãƒ",
    "shoji": "ã‚·ãƒ§ã‚¦ã‚¸",
    "shogo": "ã‚·ãƒ§ã‚¦ã‚´",
    "shoko": "ã‚·ãƒ§ã‚¦ã‚³",
    "shota": "ã‚·ãƒ§ã‚¦ã‚¿",
    "shotaro": "ã‚·ãƒ§ã‚¦ã‚¿ãƒ­ã‚¦",
    "shunichiro": "ã‚·ãƒ¥ãƒ³ã‚¤ãƒãƒ­ã‚¦",
    "shunsuke": "ã‚·ãƒ¥ãƒ³ã‚¹ã‚±",
    "shusuke": "ã‚·ãƒ¥ã‚¦ã‚¹ã‚±",
    "sohsuke": "ã‚½ã‚¦ã‚¹ã‚±",
    "soichi": "ã‚½ã‚¦ã‚¤ãƒ",
    "soichiro": "ã‚½ã‚¦ã‚¤ãƒãƒ­ã‚¦",
    "sota": "ã‚½ã‚¦ã‚¿",
    "taro": "ã‚¿ãƒ­ã‚¦",
    "tohru": "ãƒˆã‚ªãƒ«",
    "toshiro": "ãƒˆã‚·ãƒ­ã‚¦",
    "toru": "ãƒˆã‚ªãƒ«",
    "yohei": "ãƒ¨ã‚¦ãƒ˜ã‚¤",
    "yoichiro": "ãƒ¨ã‚¦ã‚¤ãƒãƒ­ã‚¦",
    "yoko": "ãƒ¨ã‚¦ã‚³",
    "yosuke": "ãƒ¨ã‚¦ã‚¹ã‚±",
    "yu": "ãƒ¦ã‚¦",
    "yudai": "ãƒ¦ã‚¦ãƒ€ã‚¤",
    "yuetsu": "ãƒ¦ã‚¦ã‚¨ãƒ„",
    "yuji": "ãƒ¦ã‚¦ã‚¸",
    "yujiro": "ãƒ¦ã‚¦ã‚¸ãƒ­ã‚¦",
    "yuko": "ãƒ¦ã‚¦ã‚³",
    "yuma": "ãƒ¦ã‚¦ãƒ",
    "yusuke": "ãƒ¦ã‚¦ã‚¹ã‚±",
    "yuta": "ãƒ¦ã‚¦ã‚¿",
    "yuto": "ãƒ¦ã‚¦ãƒˆ",
    "yuya": "ãƒ¦ã‚¦ãƒ¤",
}

# æœ«å°¾ to/do ã‚’é•·éŸ³åŒ–ã®å¯¾è±¡ã«ã™ã‚‹è‹—å­—ï¼ˆå®Œå…¨ä¸€è‡´ãƒ»å°æ–‡å­—ï¼‰
LONG_O_SURNAMES = {
    "saito", "saitoh", "saitou",
    "sato",
    "ando", "kondo", "endo", "shindo",
    "goto", "mitsudo", "kato", "sudo",
}

def roman_to_katakana(s: str, long_vowel: bool = True) -> str:
    """ãƒ­ãƒ¼ãƒå­—â†’ã‚«ã‚¿ã‚«ãƒŠï¼ˆç°¡æ˜“ï¼‰
    ä¿®æ­£ç‚¹: n + æ¯éŸ³ ã¯ã€Œãƒ³+ã‚¢ã€ã§ã¯ãªã na/ni/nu/ne/no â†’ ãƒŠ/ãƒ‹/ãƒŒ/ãƒ/ãƒ ã«ã™ã‚‹ã€‚
    """
    if not s:
        return ""
    s = s.lower().translate(MACRON)
    s = re.sub(r"[^a-z]", "", s)
    # Hepburnç³»ã® "oh" ã‚’é•·éŸ³ã® Å ã¨ã¿ãªã™ï¼ˆå­éŸ³orèªæœ«ã®ç›´å‰ã®ã¿ï¼‰
    s = re.sub(r"oh(?=[bcdfghjklmnpqrstvwxyz]|$)", "ou", s)
    out = []
    i = 0
    while i < len(s):
        # ä¿ƒéŸ³ï¼ˆãƒƒï¼‰
        if i + 1 < len(s) and s[i] == s[i + 1] and s[i] not in VOWELS and s[i] != "n":
            out.append("ãƒƒ"); i += 1; continue
        # ä¸‰æ–‡å­—ã‚³ãƒ³ãƒœ
        tri = s[i:i+3]
        if tri in TRI_MAP:
            out.append(TRI_MAP[tri]); i += 3; continue
        # äºŒã€œä¸‰æ–‡å­—ã®ç‰¹æ®Šç¶´ã‚Š
        di3 = s[i:i+3]
        if di3 in DI_MAP:
            out.append(DI_MAP[di3]); i += 3; continue
        di2 = s[i:i+2]
        if di2 in DI_MAP:
            out.append(DI_MAP[di2]); i += 2; continue
        # 'n' ã®æ‰±ã„
        if s[i] == 'n':
            if i + 1 < len(s) and s[i+1] in 'aiueo':
                key = 'n' + s[i+1]
                kana = BASE.get(key)
                if kana:
                    out.append(kana); i += 2; continue
            if i + 1 < len(s) and s[i+1] == 'y':
                out.append('ãƒ‹'); i += 1; continue
            out.append('ãƒ³'); i += 1; continue
        # å­éŸ³+æ¯éŸ³ã®åŸºæœ¬å½¢
        if i + 1 < len(s) and s[i] not in VOWELS and s[i+1] in VOWELS:
            key = ("r" if s[i] == "l" else s[i]) + s[i+1]
            kana = BASE.get(key)
            if kana:
                out.append(kana); i += 2; continue
        # æ¯éŸ³å˜ç‹¬
        if s[i] in VOWELS:
            out.append(BASE.get(s[i], "")); i += 1; continue
        i += 1
    kat = "".join(out)
    if long_vowel:
        kat = re.sub(r"ã‚ªã‚¦ã‚¦", "ã‚ªã‚¦", kat)
    return kat

def to_katakana_fullname(first: str, last: str, long_vowel: bool = True) -> str:
    return roman_to_katakana(last, long_vowel) + roman_to_katakana(first, long_vowel)

def apply_surname_long_o(ln_lower: str, ln_kana: str) -> str:
    """ç‰¹å®šè‹—å­—ã ã‘æœ«å°¾ to/do ã‚’é•·éŸ³åŒ–ï¼ˆä¾‹: Saitoâ†’ã‚µã‚¤ãƒˆã‚¦, Andoâ†’ã‚¢ãƒ³ãƒ‰ã‚¦ï¼‰ã€‚"""
    if ln_lower in LONG_O_SURNAMES:
        if ln_lower.endswith("to") and ln_kana.endswith("ãƒˆ") and not ln_kana.endswith("ãƒˆã‚¦"):
            return ln_kana + "ã‚¦"
        if ln_lower.endswith("do") and ln_kana.endswith("ãƒ‰") and not ln_kana.endswith("ãƒ‰ã‚¦"):
            return ln_kana + "ã‚¦"
    return ln_kana

def explode_authors(items: List[Dict[str, Any]]) -> pd.DataFrame:
    rows = []
    for rec in items:
        title = rec.get("title", "")
        pmid = rec.get("pmid") or rec.get("id") or ""
        journal = rec.get("journalTitle", "")
        year = rec.get("pubYear") or rec.get("firstPublicationDate", "")
        try:
            if isinstance(year, str) and len(year) >= 4:
                year = int(year[:4])
        except Exception:
            pass
        authors = (rec.get("authorList", {}) or {}).get("author", [])
        for idx, a in enumerate(authors):
            full = a.get("fullName") or ""
            first = a.get("firstName") or ""
            last = a.get("lastName") or ""
            affs = a.get("affiliation", [])
            display_name = (f"{first} {last}".strip()) if (first or last) else full
            if isinstance(affs, list):
                aff_joined = " | ".join([x for x in affs if x])
            else:
                aff_joined = affs or ""
            dept = extract_department(aff_joined)
            score = 2 if idx == 0 else 1
            rows.append(
                {
                    "PMID": pmid,
                    "Title": title,
                    "Journal": journal,
                    "Year": year,
                    "AuthorOrder": idx + 1,
                    "FullName": full,
                    "FirstName": first,
                    "LastName": last,
                    "DisplayName": display_name,
                    "Affiliation": aff_joined,
                    "MainAffiliation": pick_main_affiliation(affs),
                    "Department_guess": dept,
                    "Score": score,
                }
            )
    return pd.DataFrame(rows)

def aggregate_author_scores(df: pd.DataFrame, long_vowel: bool = True, surname_long_o: bool = False) -> pd.DataFrame:
    if df.empty:
        return df
    name_col = df["DisplayName"].where(df["DisplayName"].astype(bool), df["FullName"])
    main_aff = df["MainAffiliation"].fillna("")
    base = (
        df.assign(Author=name_col, MainAff=main_aff)
          .groupby(["Author", "MainAff"], dropna=False)["Score"]
          .sum()
          .reset_index()
          .rename(columns={"MainAff": "MainAffiliation"})
    )
    rep = (
        df.assign(Author=name_col)
          .dropna(subset=["Author"])
          .groupby("Author")
          .agg({"FirstName":"first", "LastName":"first"})
          .reset_index()
    )

    def _kana_parts(a: str):
        rec = rep[rep["Author"] == a]
        if not rec.empty:
            fn = str(rec.iloc[0]["FirstName"] or "")
            ln = str(rec.iloc[0]["LastName"] or "")
        else:
            parts = (a or "").split()
            fn = parts[0] if parts else ""
            ln = parts[-1] if len(parts) >= 2 else ""

        ln_lower = (ln or "").lower()
        ln_kana = KATAKANA_LASTNAME_OVERRIDE.get(ln_lower) or roman_to_katakana(ln, long_vowel)
        if surname_long_o and ln_lower not in KATAKANA_LASTNAME_OVERRIDE:
            ln_kana = apply_surname_long_o(ln_lower, ln_kana)

        fn_lower = (fn or "").lower()
        if fn_lower in KATAKANA_FIRSTNAME_OVERRIDE:
            fn_kana = KATAKANA_FIRSTNAME_OVERRIDE[fn_lower]
        else:
            fn_proc = re.sub(r"ryo(?=([bcdfghjklmnpqrstvwxyz]|$))", "ryou", fn_lower)
            fn_proc = re.sub(r"ryo(?=i)", "ryoui", fn_proc)
            fn_kana = roman_to_katakana(fn_proc, long_vowel)

        return ln_kana, fn_kana, (ln_kana + fn_kana)

    kana_tuple = base["Author"].apply(_kana_parts)
    base["KanaLastName"]  = kana_tuple.apply(lambda t: t[0])
    base["KanaFirstName"] = kana_tuple.apply(lambda t: t[1])
    base["KanaName"]      = kana_tuple.apply(lambda t: t[2])

    out = base.sort_values(["Score", "Author"], ascending=[False, True])
    cols = ["Author", "KanaName", "KanaLastName", "KanaFirstName", "Score"]
    return out[cols]

# --- ã‚¢ãƒ—ãƒªæœ¬ä½“ ---
if check_password():
    st.title("InsighTCROSSÂ® Literature Scorer")
    st.caption("Europe PMCï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰ã‚’åˆ©ç”¨ã—ã¦æ–‡çŒ®ã‚’æ¤œç´¢ãƒ»è‘—è€…ã‚¹ã‚³ã‚¢åŒ–ã—ã¾ã™ã€‚")

    # èªè¨¼å¾Œã®ã¿è¡¨ç¤ºã•ã‚Œã‚‹ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆãƒ­ã‚°ã‚¢ã‚¦ãƒˆï¼‹æ¤œç´¢æ¡ä»¶ï¼‰
    with st.sidebar:
        if st.button("ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ"):
            st.session_state["password_correct"] = False
            st.rerun()

        st.header("Search Filters")
        disease = st.text_input("Disease (ä¾‹: peripheral artery disease / heart failure â€»å¼•ç”¨ç¬¦ã¯ä¸è¦)")
        country = st.text_input("Country (ä¾‹: Japan)")

        col_y1, col_y2 = st.columns(2)
        with col_y1:
            year_from = st.number_input("Year From", min_value=1900, max_value=2100, value=2020, step=1)
        with col_y2:
            year_to = st.number_input("Year To", min_value=1900, max_value=2100, value=2025, step=1)

        dept = st.text_input("Department/Division in Affiliationï¼ˆä¾‹: Cardiology, å¾ªç’°å™¨å†…ç§‘ï¼‰")
        keywords = st.text_input("Keywordsï¼ˆä¾‹: Cardiology, Interventionalï¼‰")

        st.markdown("**Text availability**")
        ta_abstract = st.checkbox("Abstract")
        ta_free_full = st.checkbox("Free full text")
        ta_full_text = st.checkbox("Full text (any)")

        st.markdown("**Article type**")
        atypes = st.multiselect(
            "Select article types",
            ["Clinical Trial", "Meta-Analysis", "Randomized Controlled Trial", "Review", "Systematic Review"],
        )

        st.markdown("**Sources**")
        src_med = st.checkbox("PubMed/Medline (SRC:MED)", value=True)
        src_pmc = st.checkbox("PubMed Central (SRC:PMC)", value=False)
        src_ppr = st.checkbox("Preprints (SRC:PPR)", value=False)
        excl_ppr = st.checkbox("Exclude preprints (NOT SRC:PPR)", value=True)

        st.markdown("**Options**")
        use_synonyms = st.checkbox("MeSHã‚·ãƒãƒ‹ãƒ ã‚’å±•é–‹ã™ã‚‹ï¼ˆåºƒãæ‹¾ã†ï¼‰", value=False)
        kana_long = st.checkbox("ã‚«ã‚¿ã‚«ãƒŠï¼šé•·éŸ³ï¼ˆKoâ†’ã‚³ã‚¦ ãªã©ï¼‰ã‚’æ¨å®š", value=True)
        surname_o = st.checkbox("æ—¥æœ¬äººå§“ã®æœ«å°¾ to/doã‚’é•·éŸ³åŒ–ï¼ˆã€œãƒˆã‚¦/ãƒ‰ã‚¦ï¼‰", value=True)
        yuki_long = st.checkbox("FirstNameã€Yukiã€ã‚’ãƒ¦ã‚¦ã‚­æ‰±ã„ã«ã™ã‚‹ï¼ˆç”·æ€§åæƒ³å®šï¼‰", value=False)  # äºˆç´„ï¼šç¾è¡Œãƒ­ã‚¸ãƒƒã‚¯æœªä½¿ç”¨
        max_rows = st.number_input("Max records to fetch (safety cap)", 1000, 20000, 5000, step=500)
        relax_if_zero = st.checkbox("ğŸ” 0ä»¶ãªã‚‰è‡ªå‹•ã§æ¡ä»¶ã‚’ã‚†ã‚‹ã‚ã¦å†æ¤œç´¢", value=True)

        st.divider()
        run = st.button("ğŸ” Search & Score")

# ===== æ¤œç´¢å®Ÿè¡Œï¼ˆèªè¨¼å¾Œã®ã¿ï¼‰=====
if run:
    if not disease and not keywords and not dept:
        st.warning("å°‘ãªãã¨ã‚‚ Disease / Keywords / Department ã®ã„ãšã‚Œã‹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # ã‚¯ã‚¨ãƒªä½œæˆ
    active_query = build_query(
        disease, country, year_from, year_to, dept, keywords,
        ta_abstract, ta_free_full, ta_full_text, atypes,
        src_med, src_pmc, src_ppr, excl_ppr,
    )
    st.info(f"Query: {active_query}")

    # æ¤œç´¢
    with st.spinner("Europe PMC ã‚’æ¤œç´¢ä¸­..."):
        try:
            items, total_hits = fetch_eupmc_all(active_query, max_rows=max_rows, synonym=use_synonyms)
        except Exception as e:
            st.error(f"æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.stop()

    # 0ä»¶ãªã‚‰æ¡ä»¶ç·©å’Œã§å†æ¤œç´¢
    if len(items) == 0 and relax_if_zero:
        st.warning("0ä»¶ã ã£ãŸãŸã‚ã€Keywords / Full text / Article type ã‚’å¤–ã—ã¦å†æ¤œç´¢ã—ã¾ã™â€¦")
        active_query = build_query(
            disease, country, year_from, year_to, dept, "",
            ta_abstract, False, False, [],  # â† ç·©å’Œ
            src_med, src_pmc, False, True,  # â† ãƒ—ãƒ¬ãƒ—ãƒªãƒ³ãƒˆé™¤å¤–ã®ã¾ã¾
        )
        st.info(f"Relaxed Query: {active_query}")
        with st.spinner("æ¡ä»¶ã‚’ç·©ã‚ã¦å†æ¤œç´¢ä¸­..."):
            try:
                items, total_hits = fetch_eupmc_all(active_query, max_rows=max_rows, synonym=use_synonyms)
            except Exception as e:
                st.error(f"å†æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.stop()

    # æ¦‚è¦è¡¨ç¤º
    unique_pmids = len({(it.get("pmid") or it.get("id")) for it in items})
    try:
        med_hits = get_hit_count(f"({active_query}) AND SRC:MED", synonym=use_synonyms)
    except Exception:
        med_hits = None

    msg = f"ãƒ’ãƒƒãƒˆä»¶æ•°ï¼ˆAPIå…¨ä½“ï¼‰: {total_hits}"
    if med_hits is not None:
        msg += f"  /  PubMed/Medlineã®ã¿: {med_hits}"
    msg += f"  /  å–å¾—ï¼ˆä¸Šé™å†…ï¼‰: {len(items)}  /  è«–æ–‡ä»¶æ•°ï¼ˆå–å¾—åˆ†ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰: {unique_pmids}"
    st.success(msg)

    # è‘—è€…è¡Œã«å±•é–‹
    df = explode_authors(items)

    # å¿µã®ãŸã‚å¹´ãƒ¬ãƒ³ã‚¸ã§æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿
    def _coerce_year(x):
        try:
            return int(str(x)[:4])
        except Exception:
            return None

    if not df.empty:
        df["YearNum"] = df["Year"].apply(_coerce_year)
        df = df[(df["YearNum"].isna()) | ((df["YearNum"] >= int(year_from)) & (df["YearNum"] <= int(year_to)))]
        df = df.drop(columns=["YearNum"])

    st.subheader("Results (Author-level rows)")
    st.dataframe(df, use_container_width=True, hide_index=True)

    if not df.empty:
        # æ˜ç´°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
        st.download_button("â¬‡ï¸ Download author rows (CSV)", csv_bytes,
                           file_name="literature_author_rows.csv", mime="text/csv")

        # ã‚¹ã‚³ã‚¢é›†è¨ˆ
        agg = aggregate_author_scores(df, long_vowel=kana_long, surname_long_o=surname_o)
        st.subheader("Author Score (Full name Ã— MainAffiliation)")

        if "ConfirmKana" not in agg.columns:
            agg["ConfirmKana"] = False

        edited = st.data_editor(
            agg,
            use_container_width=True,
            hide_index=True,
            column_config={
                "ConfirmKana": st.column_config.CheckboxColumn(
                    "âœ“Kanaç¢ºèª", help="ã‚«ã‚¿ã‚«ãƒŠè¡¨è¨˜ã‚’ç›®è¦–ç¢ºèªã—ãŸã‚‰ãƒã‚§ãƒƒã‚¯", default=False
                )
            },
        )
        csv2 = edited.to_csv(index=False).encode("utf-8-sig")
        st.download_button("â¬‡ï¸ Download author scores (CSV)", csv2,
                           file_name="literature_author_scores.csv", mime="text/csv")

    st.caption(
        "æ³¨: Europe PMC ã® 'OPEN_ACCESS', 'HAS_FULL_TEXT', 'HAS_ABSTRACT', 'PUB_TYPE', 'SRC' ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ã—ã¦ã„ã¾ã™ã€‚"
        "\nãƒ’ãƒƒãƒˆä»¶æ•°ã¯ Europe PMC ã® 'hitCount' ã‚’ä½¿ç”¨ã€‚MeSHã‚·ãƒãƒ‹ãƒ å±•é–‹ã¯ 'synonym' ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ON/OFFå¯èƒ½ã§ã™ï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰ã€‚"
    )
else:
    st.info("å·¦ã®æ¡ä»¶ã‚’å…¥åŠ›ã—ã€'Search & Score' ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
